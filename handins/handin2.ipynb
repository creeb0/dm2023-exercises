{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 2 - Graph mining\n",
    "\n",
    "This handin corresponds to the topics in Week 10-15 in the course.\n",
    "\n",
    "The handin is \n",
    "* done in the chosen handin groups\n",
    "* worth 10% of the grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "Submit both files on Brightspace no later than **April 21 kl. 11.59PM**.\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of\n",
    "the number of points is **100**. For the maximum grade you need to get at least _80 points_. The minimum grade (02 in the Danish scale)\n",
    "requires **at least** 30 points, with at least 8 points on of the first three Parts (Part 1,2,3) and 6 points in the last part (Part 4).\n",
    "Good luck!\n",
    "\n",
    "**The exercise types**: There are three different types of exercises\n",
    "1. <span style='color: green'>**\\[Compute by hand\\]**</span> means that you should provide **NO code**, but show the main steps to reach the result (not all).  \n",
    "2. <span style='color: green'>**\\[Motivate\\]**</span> means to provide a short answer of 1-2 lines indicating the main reasoning, e.g., the PageRank of a complete graph is 1/n in all nodes as all nodes are symmetric and are connected one another.\n",
    "3. <span style='color: green'>**\\[Describe\\]**</span> means to provide a potentially longer answer of 1-5 lines indicating the analysis of the data and the results. \n",
    "4. <span style='color: green'>**\\[Prove\\]**</span> means to provide a formal argument and NO code. \n",
    "5. <span style='color: green'>**\\[Implement\\]**</span> means to provide an implementation. Unless otherwise specified, you are allowed to use helper functions (e.g., ```np.mean```, ```itertools.combinations```, and so on). However, if the task is to implement an algorithm, by no means a call to a library that implements the same algorithm will be deemed as sufficient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN IMPORTS - DO NOT TOUCH!\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#!{sys.executable} -m pip install matplotlib\n",
    "#!{sys.executable} -m pip install networkx\n",
    "#!{sys.executable} -m pip install torchvision\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from utilities.load_data import load_mnist\n",
    "import utilities.email as email\n",
    "from utilities.mnist import *\n",
    "\n",
    "from utilities.make_graphs import read_edge_list, read_list, load_data\n",
    "\n",
    "### END IMPORTS - DO NOT TOUCH!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Random walks and PageRank  (12 points)\n",
    "\n",
    "In this exercise recall that the PageRank is defined as \n",
    "\n",
    "$$\\mathbf{r} = \\alpha \\mathbf{Mr} + (1-\\alpha)\\mathbf{p}$$ \n",
    "\n",
    "where $\\mathbf{r}\\in \\mathbb{R}^n$ is the PageRank vector, $\\alpha$ is the restart probability, $\\mathbf{M} = A\\Delta^{-1}$, and $\\mathbf{p}$ is the restart (or personalization) vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************\n",
    "### Task 1.1.1 (4 points)\n",
    "What is the PageRank of a **$d$-regular** graph with $n$ nodes and $\\alpha=1$? \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> your answer without showing the exact computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (6 points)\n",
    "Look at the graph below (run the code) and try make a guess about the PageRank values of each node only by reasoning on the graph's connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from([(1,2),(2,3), (2,4), (3,4), (1,3)])\n",
    "nx.draw(G, with_labels=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: green'>**A) \\[Implement\\]**</span> the PageRank for $\\alpha=1$ for the graph using the Power Iteration method (use $\\epsilon=1e-16$ to stop the iteration).<br> \n",
    "\n",
    "<span style='color: green'>**B) \\[Implement\\]**</span> Plot the norm square difference of the $r$ vector (between any two consecutive iterations) for each iteration.\n",
    "\n",
    "<span style='color: green'>**C) \\[Motivate\\]**</span> Do you observe a constant decrease of the norm square difference as iterations are increasing, and is this decrease expected or not?\n",
    "\n",
    "<span style='color: green'>**D) \\[Implement\\]**</span> the PageRank for $\\alpha=1$ using the eigenvector method.<br> \n",
    "\n",
    "<span style='color: green'>**E) \\[Motivate\\]**</span> Are solutions of both methods the same? Why don't we only use the eigenvector method that optimally solves the problem? \n",
    "\n",
    "<span style='color: green'>**F) \\[Motivate\\]**</span> Do the real vector match with your first guess? Can you see a pattern between the pagerank score of each node and its edges?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A) YOUR CODE HERE \n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def my_pagerank(G, alpha=1, tol=1e-16):\n",
    "    A = nx.adjacency_matrix(G)\n",
    "\n",
    "    n = len(G.nodes())\n",
    "\n",
    "    r = np.ones(n) / n\n",
    "\n",
    "    while True:\n",
    "        r_new = alpha * r.dot(A.toarray()) + (1 - alpha) * np.ones(n) / n\n",
    "        if np.linalg.norm(r_new - r, ord=1) < tol:\n",
    "            break\n",
    "        r = r_new\n",
    "    \n",
    "    return r_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = my_pagerank(G)\n",
    "print(pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) YOUR CODE HERE \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1,2),(2,3), (2,4), (3,4), (1,3)])\n",
    "\n",
    "def my_pagerank_diff(G, alpha=1, tol=1e-16):\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    n = len(G.nodes())\n",
    "    r = np.ones(n) / n\n",
    "\n",
    "    norm_diff = []\n",
    "    \n",
    "    while True:\n",
    "        r_new = alpha * r.dot(A.toarray()) + (1 - alpha) * np.ones(n) / n\n",
    "        diff = np.linalg.norm(r_new - r, ord=2)**2\n",
    "        norm_diff.append(diff)\n",
    "        if diff < tol:\n",
    "            break\n",
    "        r = r_new\n",
    "    \n",
    "    plt.plot(norm_diff)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Norm square difference\")\n",
    "    plt.title(\"Norm square difference of r vector between consecutive iterations\")\n",
    "    plt.show()\n",
    "    \n",
    "    return r_new\n",
    "\n",
    "pagerank = my_pagerank_diff(G)\n",
    "print(pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> C) The decrease is not exactly constant. This decrease is expected as the number of iterations increase as the power iteration method has been designed such that it converges to the PageRank vector. As the program approaches this vector, the norm square difference between each iteration is expected to decrease. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D) YOUR CODE HERE\n",
    "\n",
    "def my_pagerank_eigenvector(G, alpha=1):\n",
    "    A = nx.adjacency_matrix(G).toarray()\n",
    "    n = len(G.nodes())\n",
    "\n",
    "    d = np.sum(A, axis=1)\n",
    "    M = A / d[:, np.newaxis]\n",
    "    M = np.nan_to_num(M)\n",
    "\n",
    "    I = np.identity(n)\n",
    "    v = np.ones(n) / n\n",
    "    M_hat = alpha * M + (1 - alpha) * np.ones((n,n)) / n\n",
    "    w, v = np.linalg.eig(M_hat.T)\n",
    "    idx = np.argmax(w)\n",
    "    pagerank = v[:,idx].real\n",
    "    pagerank /= sum(pagerank)\n",
    "\n",
    "    return pagerank\n",
    "\n",
    "pagerank = my_pagerank_eigenvector(G)\n",
    "print(pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> E) The solutions are not the same. The power iteration method is faster and takes up less memory space compared to the eigenvector, hence is better for larger graphs.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> F) YOUR ANSWER HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "Assume you have embedded the graph in **1.1.2** with a __Linear Embedding__ using unormalized Laplacian matrix of the graph as the similarity matrix. How do you expect the embeddings to be if the embedding dimension is $d = 1$?\n",
    "(1) Check the correct box below and (2) motivate your answer.\n",
    "\n",
    "* [ ] Nodes 1, 2, 3, 4 will be placed in the corners of a hypercube\n",
    "* [ ] Nodes 2,3 will have the same embedding while 1,4 will be far from each other.\n",
    "* [x] Nodes 1,4 and 2,3 will have very close embeddings.\n",
    "* [ ] Nodes 3,4 will be very far apart.\n",
    "\n",
    "<font color='red'>IMPORTANT: Do NOT just choose one answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">As there are no nodes with zero degree and the graph has only one connected component, the first eigenvector in the Laplacian matrix will be a constant vector. Hence, we use the second smallest eigenvector which is the Fiedler vector. The cutpoints are the edges (1,2) and (2,4), thus 2 and 3 will have the same embedding since they belong in the same set are connected by an edge. As 1 and 4 are connected by an edge and belong in the same set (different than 2 and 3) as well, they too will have very close embeddings.</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Spectral Properties of the Graph Laplacian (17 points)\n",
    " <span style='color: green'>**\\[Prove\\]**</span> **the following properties:**\n",
    " You will be given points for each of the properties that you prove, rather than points for the exercise as a whole.\n",
    "\n",
    "**Note that all question correspond to the eigenvalues of the LAPLACIAN (NOT THE NORMALIZED)**\n",
    "\n",
    "For a graph with $n$ nodes the eigenvalues of the LAPLACIAN ($L  = D - A$) is sorted in ascending order, i.e.,<br>\n",
    "**$\\lambda_0\\leq\\lambda_1\\leq...\\leq\\lambda_{n-1}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.1 (1 points)\n",
    "\n",
    "For all graphs $\\lambda_0 = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.2 (2 points)\n",
    "For the complete graph, $\\lambda_1, \\dots, \\lambda_{n-1} = n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.3 (3 points)\n",
    "\n",
    "For all the graphs with $k$ connected components $\\lambda_0 = \\lambda_1 =...=\\lambda_ k = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">Since the Laplacian matrix L is symmetric and positive semi-definite, all its eigenvalues are non-negative. Since the graph has k connected components, L can be expressed as a k block diagonal matrix with k blocks corresponding to the respective connected components. Let L1, L2, ... , Lk be the Laplacian matrices of the k blocks. We can see that the eigenvalues of L is hence the union set of the respective eigenvalues of the individual L1, L2, ..., Lk of the k components. Since each component has at least one node, its respective Laplacian matrix will contain an eigenvalue 0 corresponding to its constant vector of 1's. Thus, each block will have eigenvalue 0 and $\\lambda_0 = \\lambda_1 =...=\\lambda_ k = 0$.  </span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.4 (5 points)\n",
    "Given a graph $G$ with eigenvalues of the laplacian $\\lambda_0, \\lambda_1,...,\\lambda_{n-1}$.<br>\n",
    "We randomly remove an edge from $G$ and we re-calculate the eigenvalues as $\\lambda'_0, \\lambda'_1,...,\\lambda'_{n-1}$.<br>\n",
    "Can we have $\\lambda'_{i}>\\lambda_{i}$ for some $0\\leq i\\leq n-1$? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">Laplacian matrix of a graph G: L=D-A; D is the diagonal matrix of vertex degrees & A is the adjacency matrix. Removing an edge from G, we obtain the new adjacency matrix A' by subtracting the corresponding entry from the original adjacency matrix A. This will change L to L' = D - A'. Assuming that the edge removed connects vertices i and j in G, the corresponding entries in the adjacency matrix A were A[i][j] = 1 and A[j][i] = 1.</span>\n",
    "\n",
    "<span style=\"color:red\">L' can be written as L' = D - A' = D - (A - E[i][j] - E[j][i]), where E[i][j] and E[j][i] have all entries zero except for a 1 in the (i,j) and (j,i) positions, respectively.</span>\n",
    "\n",
    "<span style=\"color:red\">Let's assume that the eigenvalue $\\lambda_i$ is the i-th smallest eigenvalue of L. Then, $\\lambda'_i = \\lambda_i$ + d, where d is a non-negative number that depends on the degree of vertices i and j in G. If both degrees are 1, d = 1. If the degree of vertex i = 1 and the degree of vertex j > 1, then d = 1/deg(j). Similarly, if the degree of vertex j = 1 and the degree of vertex i > 1, then d = 1/deg(i). If the degrees of both vertices i and j are greater than 1, then d = 2/(deg(i) + deg(j)).</span>\n",
    "\n",
    "<span style=\"color:red\">Thus, if d is large enough, we could have $\\lambda'_{i}>\\lambda_{i}$. This could be when the degrees of vertices i and j are large and the edge removed connects two highly connected vertices in the graph.</span>\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.5 (6 points)\n",
    "Suppose that the graph $G$ consists of two connected componentes of equal size named  $G_1$ and $G_2$. For simplicity assume that $n$ is even.<br>\n",
    "The Laplacian of $G_1$ has eigenvalues $\\lambda^1_0,\\lambda^1_1,...,\\lambda^1_{n/2-1}$.<br>\n",
    "The Laplacian of $G_2$ has eigenvalues $\\lambda^2_0,\\lambda^2_1,...,\\lambda^2_{n/2-1}$.<br>\n",
    "Prove that the  Laplacian of $G$ is consisted of the eigenvalues of the Laplacians of $G_1$ and $G_2$ in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">Let L1 & L2 be the Laplacian matrixes of G1 and G2 respectively. Then the Laplacian matrix L of G can be written as L = [[L1, 0] , [0, L2]] where 0 is the zero matrix of size n/2. Let $\\lambda_i$ be the i-th eigenvalue of L and $v_i$ be the corresponding eigenvector. Then, L$v_i$ = $\\lambda_i$$v_i$. Let $u_i$ and $w_i$ be the first n/2 and last n/2 entries of $v_i$, respectively. Then we can write $v_i$ = [$u_i$, $w_i$]. Hence, we can rewrite L$v_i$ = $\\lambda_i$$v_i$ to be [[L1, 0] , [0, L2]][$u_i$, $w_i$] = $\\lambda_i$[$u_i$, $w_i$] which gives us L1$u_i$ = $\\lambda_i$$u_i$ & L2$w_i$ = $\\lambda_i$$w_i$. We find that $u_i$ & $w_i$ are eigenvectors of L1 & L2 respectively, with eigenvalues of $\\lambda_i$. </span>\n",
    "\n",
    "<span style=\"color:red\">Since eigenvalues of L1 and L2 are ascending, we get 0 $\\leqslant\\lambda^1_0\\leqslant\\lambda^1_1\\leqslant...\\leqslant\\lambda^1_{n/2-1}$ and 0 $\\leqslant\\lambda^2_0\\leqslant\\lambda^2_1\\leqslant...\\leqslant\\lambda^2_{n/2-1}$</span>\n",
    "\n",
    "<span style=\"color:red\">Hence, we can write the eigenvalues of L as a union of the eigenvalues of L1 & L2 as follows: {$\\lambda_i$} = {$\\lambda^1_0,\\lambda^1_1,...,\\lambda^1_{n/2-1}$} $\\cup$ {$\\lambda^2_0,\\lambda^2_1,...,\\lambda^2_{n/2-1}$}. Since the eigenvalues of L1 and L2 areascending, the eigenvalues of L are also sorted in ascending order.</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Graphs and Spectral clustering\n",
    "In this part, you will experiment and reflect on spectral clustering as a technique for partitioning a graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: $\\varepsilon$-neighbourhood graph (10 points)\n",
    "\n",
    "In this subsection you will experiment with some biological data [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003268](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003268).\n",
    "\n",
    "**!IMPORTANT!** First run the following code to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "from utilities.make_graphs import read_edge_list, read_list, load_data\n",
    "import numpy as np\n",
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "### Task 2.1.1 (4 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the $\\varepsilon$-neighborhood graph, using Eucledian ($L_2$) distance. \n",
    "\n",
    "**Note**: Be sure that your constructed graph does not contain self-loop edges (edges from i to i for each i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "# Be sure that your constructed graphs does not \n",
    "# contain loop edges (edges from i to i for some node i)\n",
    "\n",
    "def nn_graph(data, eps, remove_self=True, directed=False):\n",
    "    n = len(X)\n",
    "    G = nx.Graph()    \n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    dist_matrix = np.linalg.norm(data[:, None] - data, axis=-1)\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors = np.where(dist_matrix[i] <= eps)[0]\n",
    "        for j in neighbors:\n",
    "            if i != j:  # avoid self-loops\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    if remove_self:\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    \n",
    "    ### YOUR ENDS CODE HERE\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (2 points)\n",
    "\n",
    "Try with different $\\varepsilon$ values (select a small set of $\\varepsilon$, e.g., 0.01-0.5 values) and plot the graphs. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> what you observe as epsilon increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the code below\n",
    "eps_values = [0.01, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "for eps in eps_values: \n",
    "    ax=plt.subplot()\n",
    "    ax1=plt.subplot()\n",
    "    G = nn_graph(X, eps)\n",
    "    pos=nx.spring_layout(G)  \n",
    "    nx.draw_networkx_edges(G,pos=X)\n",
    "    nx.draw_networkx_nodes(G, pos=X, node_color=Y, node_size=20, cmap=plt.get_cmap('tab10'))\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">The larger the epsilon value, the fewer edges are observed in the graph. This is due to the fact that the graph is constructed based on the epsilon value as a proximity threshold. The graph will construct edges to all nodes within a distance of epsilon to the initial node. As epsilon increases, there will be fewer nodes wihtin the distance of epsilon to the initial node, resulting in fewer edges being connected to the intial node. </span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (2 points)\n",
    "Assign to each edge in the $\\varepsilon$-neighborhood graph a weight\n",
    "\n",
    "$$W_{i j}=e^{-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{t}}$$\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the function ```weighted_nn_graph``` below that returns the weighted graph given the data matrix in input and the values eps and $t$, where $t$ is the parameter of the equation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_nn_graph(data, eps=20, t=0.1):\n",
    "    n = len(data)\n",
    "    G = nx.Graph()\n",
    "    ### YOUR CODE STARTS HERE\n",
    "  \n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (2 points)\n",
    "\n",
    "Vary $t \\in \\{10, 0.1, 0.000001\\}$. Plot the weights as a histogram using the code below in order to analyse the results using the provided code.</br>\n",
    "What happens when $t$ is very small, close to $0$, i.e., $t \\rightarrow 0$?</br> What happens when $t$ is very large?\n",
    "</br>Is the behaviour with $t = 0$ expected?\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> your answer reasoning on the formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [10, 0.2, 0.07, 0.000001]\n",
    "fig, ax = plt.subplots(1,4, figsize=(20, 10))\n",
    "row = 0\n",
    "\n",
    "for i, t in enumerate(ts):\n",
    "    G = weighted_nn_graph(X, eps=60, t=t)\n",
    "    ys = []\n",
    "    \n",
    "\n",
    "    col = i \n",
    "    for i, d in enumerate(G.edges.data()):\n",
    "        ys.append(d[2]['weight'])\n",
    "    plt.hist(ys, bins=100)\n",
    "    ax[col].hist(ys, bins=100)\n",
    "    ax[col].set_title(\"t: \"+str(t))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Spectral clustering (20 points)\n",
    "\n",
    "We will now look at spectral clustering and its properties. <br>\n",
    "For this Task we will use a subgraph from [malaria_genes](https://networks.skewed.de/net/malaria_genes). <br>\n",
    "Note that this dataset is the biological network of the data used in Task 2.1. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.1 (5 point)\n",
    "Run the code below to load and visualize the network. <br> \n",
    "By only observing the below plot and the $nn$-plots (nearest-neighbor plots)of task 2.1.2, which $\\varepsilon$ values seems to better approximate the real network? (just think of the answer you don't have to write something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = read_edge_list('./data/edges.txt')\n",
    "n = np.max(edgelist)+1\n",
    "G = nx.Graph()\n",
    "for i in range(n):\n",
    "    G.add_node(i)\n",
    "for edge in edgelist:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "pos=nx.spring_layout(G)  \n",
    "nx.draw_networkx_edges(G,pos=X)\n",
    "nx.draw_networkx_nodes(G, pos=X, node_color=Y, node_size=20, cmap=plt.get_cmap('tab10'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are having the real network, lets check how good $nn$-graph (and for which $\\epsilon$ value) is a good \"approximates\"the real graph. <br> \n",
    "\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> a function that calculates the absolute edge difference between the real network $G$ and the one $\\epsilon$-neighborhood graph. Note that in order to do that you have to follow two steps: \n",
    "1. In the first step you have to check if an edge in the real graph is also presented in the $nn$-graph, if not you increase the counter\n",
    "2. In the second step you follow the opposite direction, that is you check if for every edge of the $nn$-graph if is also presented in the original one, if not you increase the counter. (Faster way just use the adjacency matrices) <br>\n",
    "\n",
    "B) <span style='color: green'>**\\[Implement\\]**</span> Plot the edge-difference plot for the range of epsilon values in the range[0.01, 0.11] with step = 0.005. <br>\n",
    "C) <span style='color: green'>**\\[Motivate]**</span> By observing the plot it seems that there exists only one global minimum and no local minimum. Try to prove/disprove this intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A) YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**C) YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (2 points)\n",
    "Compute the eigenvectors and eigenvalues (using the provided function) of the Normalized Laplacian and the Random Walk Laplacian of the graph $G$.<br> \n",
    "Plot the spectrum (eigenvalues).\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the code to compute the different Laplacians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_eig(L): \n",
    "    \"\"\"\n",
    "        Takes a graph Laplacian and returns sorted the eigenvalues and vectors.\n",
    "    \"\"\"\n",
    "    lambdas, eigenvectors = np.linalg.eig(L)\n",
    "    lambdas = np.real(lambdas)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas = lambdas[order]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "    \n",
    "    return lambdas, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_norm = None\n",
    "L_rw = None\n",
    "\n",
    "### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE\n",
    "\n",
    "eigval_norm, eigvec_norm = graph_eig(L_norm)\n",
    "eigval_rw, eigvec_rw = graph_eig(L_rw)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(eigval_norm, 'b-o', label='Spectrum of Normalized Laplacian', )\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "plt.plot(eigval_rw, 'b-o', label='Spectrum of the Random Walk Laplacian')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.3 (4 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the function ```spect_cluster``` that returns a vector ```y_clust``` in which each entry y_clust\\[i\\] represents the community assigned to node $i$. The method should be able to handle both the Normalized Laplacian, and the Random Walk Laplacian. You are allowed to use your implementation from the weekly exercises and ```sklearn.cluster.k_means``` for k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import k_means\n",
    "\n",
    "def spect_cluster(G, eig_type=\"normal\", k=5, d=5):\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return y_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G, clusters):\n",
    "    plt.figure(1,figsize=(30,15))\n",
    "    nodes = G.nodes()\n",
    "    ec = nx.draw_networkx_edges(G, X, alpha=0.2)\n",
    "    nc = nx.draw_networkx_nodes(G, X, nodelist=nodes, node_color=clusters, node_size=100, cmap=plt.cm.jet)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "your_clusters = spect_cluster(G, k=6)   \n",
    "plot_graph(G, your_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.4 (1 points)\n",
    "\n",
    "Finally, use your implementation of spectral clustering with different Laplacians and different values of $k \\in [2,7]$ and plot the results using the helper function ```plot_graph```. \n",
    "\n",
    "<span style='color: green'>**\\[Describe\\]**</span> the results you obtain. Especially, what is the difference between the Random Walk and the Normalized Laplacians, if any? How do you explain such differences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['normal', 'random']: \n",
    "    for k in np.arange(2,8): \n",
    "        your_clusters = spect_cluster(G,eig_type=method, k=k)\n",
    "        plot_graph(G, your_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.5 (4 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the modularity. Recall that the definition of modularity for a set of communities $C$ is\n",
    "$$ \n",
    "Q=\\frac{1}{2 m} \\sum_{c \\in C} \\sum_{i \\in c} \\sum_{j \\in c}\\left(A_{i j}-\\frac{d_{i} d_{j}}{2 m}\\right) \\qquad \\qquad (1) \n",
    "$$\n",
    "where $A$ is the adjacency matrix, and $d_i$ is the degree of node $i$\n",
    "\n",
    "**Note**: Use ```plot_graph``` function in order to see for yourself if maximising modularity leads a better clustering. If you did not succeed with the previous Task, you are allowed to use [Scikit Learn Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G, clustering):\n",
    "    modularity = 0\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    m = G.number_of_edges()\n",
    "\n",
    "    for c in set(clustering.values()):\n",
    "        nodes_c = [n for n in clustering.keys() if clustering[n] == c]\n",
    "        subgraph = G.subgraph(nodes_c)\n",
    "        intdeg = subgraph.number_of_edges() / (2*m)\n",
    "        extdeg = sum(G.degree(n) for n in nodes_c) / (2*m)\n",
    "        modularity += intdeg - extdeg**2\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.6 (2 points)\n",
    "\n",
    "Compute the modularity of your Spectral Clustering Implementation for different values of $k$. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Which $k$ value maximizes the modularity? From your perspective, does spectral clustering forms \"clear\" clusters for the best $k$ found by modularity? Using the spectral graph theory, why do you think this is/isn't the case? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = []\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "for k in ks:\n",
    "    clusters = spect_cluster(G, k=k) ### NOTE: If you do not use your implementation substitute with a call to the sklearn one. \n",
    "    mods.append(modularity(G, clusters))\n",
    "\n",
    "# You may want to use plt.plot to plot the modularity for different values of k\n",
    "plt.plot(ks, mods)\n",
    "print(mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">k - value</span>\n",
    "\n",
    "<span style=\"color:red\">From my perspective, spectral clustering could form \"clear\" clusters for the best k found by modularity but it is not always the case as it depends on several factors.</span>\n",
    "\n",
    "<span style=\"color:red\">Spectral clustering uses eigenvectors of a Laplacian graph to find low-dimensional representations of the given data to preserve its structure. The code will first construct a similarity graph and then compute the Laplacian matrix before applying eigenvalue decomposition to obtain corresponding eigenvectors to the k smallest eigenvalues. The data is then clustered into k groups based on these eigenvalues. Hence, the quality of results depends heavily on the k chosen. If k is too small, the clusters might not be well defined enough to show the structure of the data. But if k is too large, data may be overfit and there may not be any meaningful interpretation of results. Thus, when data is noisy, high-dimensional or contains overlapping clusters, spectral clustering might not be able to form \"clear\" clusters.</span>\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.7 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> There seems to be a relationship between graph embeddings and spectral clustering, can you guess that? _Hint_: Think to the eigenvectors of the graph's Laplacians. (1) Check the correct box below and (2) motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [x] If the embeddings are linear and the similarity is the Laplacian, the embeddings we obtain minimizing the L_2 norm are equivalent to the eigenvectors of the Laplacian. \n",
    "* [ ] If the embeddings are random-walk-based embeddings, the eigenvectors of the Random Walk Laplacian are related to the embeddings obtained by such methods. \n",
    "* [ ] The relationship is just apparent. \n",
    "* [ ] If the embeddings are linear and the similarity is the Adjacency matrix, the eigenvectors of the Laplacian are equivalent to the embeddings. \n",
    "\n",
    "<font color='red'>IMPORTANT: Do NOT just choose one answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">According the the spectral embedding theorem, the eigenvectors of the normalized Laplacian graph can be used as coordinates to embed the nodes of a graph in a low-dimensional Euclidean space. More specifically, the i-th coordinate of the embedding of node j is given by the i-th element of the eigenvector corresponding to the i-th smallest eigenvalue in the Laplacian matrix. The embeddings we obtain minimizing the L_2 norm are also equivalent to the eigenvectors of the Laplacian matrix and is also known as the Laplacian eigenmap. This is only true when similarity is based on Laplacian matrix and not for other types of embeddings or measures.</span>\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Link analysis\n",
    "In this exercise, we will work with PageRank, Random Walks and their relationships with graph properties. \n",
    "We will use the most generic definition\n",
    "\n",
    "$$\\mathbf{r} = \\alpha \\mathbf{Mr} + (1-\\alpha)\\mathbf{p}$$\n",
    "\n",
    "with $\\mathbf{r}$ the PageRank vector, $\\mathbf{M}$ the weighted transition matrix, and $\\mathbf{p}$ the personalization vector. \n",
    "Additionally, let $n = |V|$, where $V$ is the nodes in the graph above.\n",
    "Remember that in the case of PageRank the entries of the personalization vector are $p_i = 1/n$ for all $i$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 Approximate PageRank (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1.1 (3 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span>  a different algorithm for computing Personalized PageRank. This algorithm runs a fixed number of iterations and uses the definition of random walks. \n",
    "At each step, the algorithm either selects a random neighbor with probability $\\alpha$ or returns to the starting node with probability $1-\\alpha$. Every time a node is visited a counter on the node is incremented by one. Initially, each counter is 0. The final ppr value is the values in the nodes divided by the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def approx_personalized_pagerank(G, node, alpha = 0.85, iterations = 1000): \n",
    "    ppr = np.zeros(G.number_of_nodes())\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return ppr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (3 points)\n",
    "\n",
    "Run the ```approx_personalized_pagerank``` with default $\\alpha$ and iterations $\\{10, n, 2n, 4n, 100n, 1000n\\}$ where $n$ is the number of nodes in the graph and starting node the node with the highest PageRank computed in Task 3.1.2.\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> what you notice as the number of iterations increase. Why are the values and the top-10 nodes ranked by PPR changing so much? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = read_edge_list('./data/edges.txt')\n",
    "n = np.max(edgelist)+1\n",
    "G = nx.Graph()\n",
    "for i in range(n):\n",
    "    G.add_node(i)\n",
    "for edge in edgelist:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "starting_node = np.argmax(nx.pagerank(G))\n",
    "for i, iterations in enumerate([10, G.number_of_nodes(), G.number_of_nodes()*2, G.number_of_nodes()*4, G.number_of_nodes()*100, G.number_of_nodes()*1000]):\n",
    "    r = approx_personalized_pagerank(G, starting_node, iterations = iterations)\n",
    "    r[starting_node] = 0\n",
    "    r_sorted = np.argsort(r)[::-1]\n",
    "    r_values = np.sort(r)[::-1]\n",
    "    print(f'Iteration {iterations}: top-10 r={r_sorted[:10]}\\n top-10 values={r_values[:10]}\\n')\n",
    "\n",
    "import operator\n",
    "rr = nx.pagerank(G, alpha=0.85, personalization = {starting_node: 1})\n",
    "rr[starting_node] = 0\n",
    "r=np.zeros(len(rr))\n",
    "for k in rr: r[k] = rr[k]\n",
    "r_sorted = np.argsort(r)[::-1]\n",
    "r_values = np.sort(r)[::-1]\n",
    "print(f'top-10 r={r_sorted[:10]}\\n top-10 values={r_values[:10]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (2 points)\n",
    "\n",
    "Compare the 5 nodes with the highest PPR obtained from ```nx.pagerank(G, alpha=0.85, personalization={node_highest_pagerank: 1})``` and the one obtained by the approximation. \n",
    "\n",
    "<span style='color: green'>**\\[Describe\\]**</span> the differences. Does the number of iterations affect the results? Is there a relationship between the number of iterations and the results? Is there a relationship between the approximated value of PageRank and the real value? Do you notice anything as the number of iteration increases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "ppr_nx = nx.pagerank(G, alpha=0.85, personalization = {starting_node: 1})\n",
    "r_nx = [0 for _ in range(G.number_of_nodes())]\n",
    "for k, v in ppr_nx.items():\n",
    "    r_nx[k] = v\n",
    "r_est = approx_personalized_pagerank(G, starting_node, alpha=0.85)\n",
    "\n",
    "topk_nx = np.argsort(r_nx)[-5:]\n",
    "topk_est = np.argsort(r_est)[-5:]\n",
    "\n",
    "print(topk_nx, topk_est)\n",
    "\n",
    "for iterations in [10, G.number_of_nodes(), G.number_of_nodes()*2, G.number_of_nodes()*4, G.number_of_nodes()*100, G.number_of_nodes()*1000]:\n",
    "    print(f'Number of iterations {iterations}')\n",
    "    ppr_nx = nx.pagerank(G, alpha=0.85, personalization = {starting_node: 1})\n",
    "    r_nx = [0 for _ in range(G.number_of_nodes())]\n",
    "    for k, v in ppr_nx.items():\n",
    "        r_nx[k] = v\n",
    "    r_est = approx_personalized_pagerank(G, starting_node, iterations = iterations, alpha=0.85)\n",
    "    print(f'Approximate PPR: {r_est[:10]}')\n",
    "    print(f'Real PPR: {r_nx[:10]}')\n",
    "    \n",
    "    \n",
    "    topk_nx = np.argsort(r_nx)[-5:]\n",
    "    topk_est = np.argsort(r_est)[-5:]\n",
    "\n",
    "    print(f\"Topk of nx.pagerank: {topk_nx}, Topk of our estimation {topk_est}, Size of intersection: {len(set(topk_nx).intersection(set(topk_est)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1.4 (2 points)\n",
    "\n",
    "Run again the same experiment but this time use $\\alpha = 0.1$. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Motivate whether and why you need more or less iterations to predict the 5 nodes with the highest PPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterations in [10, G.number_of_nodes(), G.number_of_nodes()*2, G.number_of_nodes()*4, G.number_of_nodes()*100]:\n",
    "    ppr_nx = nx.pagerank(G, alpha=0.1, personalization = {starting_node: 1})\n",
    "    r_nx = [0 for _ in range(G.number_of_nodes())]\n",
    "    for k, v in ppr_nx.items():\n",
    "        r_nx[k] = v\n",
    "    r_est = approx_personalized_pagerank(G, starting_node, iterations = iterations, alpha=0.1)\n",
    "\n",
    "    topk_nx = np.argsort(r_nx)[-5:]\n",
    "    topk_est = np.argsort(r_est)[-5:]\n",
    "\n",
    "    print(f\"Topk of nx.pagerank: {topk_nx}, Topk of our estimation {topk_est}, Size of intersection: {len(set(topk_nx).intersection(set(topk_est)))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 Spam and link farms (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now study the effect of spam in the network and construct a link farm. In this part, if you want to modify the graph, use a copy of the original graph every time you run your code, so that you do not keep adding modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = read_edge_list('./data/edges.txt')\n",
    "n = np.max(edgelist)+1\n",
    "G2 = nx.Graph()\n",
    "for i in range(n):\n",
    "    G2.add_node(i)\n",
    "for edge in edgelist:\n",
    "    G2.add_edge(edge[0], edge[1])\n",
    "G = G2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2.1 (3 points)\n",
    "Based on the analysis in the slides, construct a spam farm $s$ on the graph $G$ with $T$ fake nodes. Assume that $s$ manages to get links from node 1. With $\\alpha=0.5$, \n",
    "\n",
    "<span style='color: green'>**\\[Describe\\]**</span> which is the minimum number of pages $T$ that we you need to add in order to get $s$ being assigned the highest PageRank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE IF YOU NEED THAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2.2 (3 points)\n",
    "In the above scenario, assume that $T = \\frac{1}{5}$ of the nodes in the original graph. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> what value of $\\alpha$ will maximize the PageRank $\\mathbf{r}_s$ of the link farm $s$. Provide sufficient justification for your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2.3 (3 points)\n",
    "\n",
    "Now we fix both $\\alpha = 0.5$ and $T = \\frac{1}{5}n$. \n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span>  ```trusted_pagerank``` the method for spam mass estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trusted_pagerank(G, trusted_indices, iterations=500, alpha=0.5):\n",
    "    r = None   \n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2.4 (3 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> whether we are able to detect the node $s$, if the trusted set of nodes is a random sample $10\\%$ of the nodes in the original graph. \n",
    "If not, what could be a viable solution? Which nodes would you rather choose as trusted?\n",
    "\n",
    "You are not obliged to, but you can write some helper code to reach the answer faster.  \n",
    "\n",
    "_Hint_: Remember the spam mass formula in the Link Analysis lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Graph embeddings (19 points)\n",
    "In this final part, we will try a different approach for clustering the data from above. \n",
    "The strategy is going to be the following:\n",
    "\n",
    "1. Use VERSE [[1]](https://arxiv.org/pdf/1803.04742.pdf) to produce embeddings of the nodes in the graph.\n",
    "2. Use K-Means to cluster the embeddings. Measure and report NMI for the clustering. \n",
    "\n",
    "[[1](https://arxiv.org/pdf/1803.04742.pdf)] Tsitsulin, A., Mottin, D., Karras, P. and Müller, E., 2018, April. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference (pp. 539-548)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = email.S_dir.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1.1 (6 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the methods below to compute sampling version of VERSE. _Hint:_ it might be a help to look in the original article \\[1\\] above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ''' Return the sigmoid function of x \n",
    "        x: the input vector\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return x\n",
    "\n",
    "def pagerank_matrix(G, alpha = 0.85) :     \n",
    "    ''' Return the Personalized PageRank matrix of a graph\n",
    "\n",
    "        Args:\n",
    "            G: the input graph\n",
    "            alpha: the dumping factor of  PageRank\n",
    "\n",
    "        :return The nxn PageRank matrix P\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return P\n",
    "    \n",
    "\n",
    "def update(u, v, Z, C, step_size) :\n",
    "    '''Update the matrix Z using row-wise gradients of the loss function\n",
    "\n",
    "       Args:\n",
    "            u : the first node\n",
    "            v : the second node\n",
    "            Z : the embedding matrix\n",
    "            C : the classification variable used in Noise Contrastive estimation indicating whether the sample is positive or negative\n",
    "            step_size: step size for gradient descent\n",
    "\n",
    "\n",
    "       :return nothing, just update rows Z[v,:] and and Z[u,:]\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    \n",
    "    \n",
    "def verse(G, S, d, k = 3, step_size = 0.0025, steps = 10000): \n",
    "    ''' Return the sampled version of VERSE\n",
    "\n",
    "        Args:\n",
    "            G: the input Graph\n",
    "            S: the PageRank similarity matrix\n",
    "            d: dimension of the embedding space\n",
    "            k: number of negative samples\n",
    "            step_size: step size for gradient descent\n",
    "            steps: number of iterations\n",
    "\n",
    "        :return the embedding matrix nxd\n",
    "    '''\n",
    "    n = G.number_of_nodes()\n",
    "    Z = 1/d*np.random.rand(n,d)\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs the `verse` algorithm above on G and stores the embeddings to 'verse.npy'\n",
    "P   = pagerank_matrix(G)\n",
    "emb = verse(G, P, 128, step_size=0.0025, steps=10_000)\n",
    "np.save('verse.npy', emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.2 (3 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a small piece of code that runs $k$-means on the embeddings with $k \\in [2,7]$ to evaluate the performance compared to Spectral clustering using the NMI as measure. You can use ```sklearn.metrics.normalized_mutual_info_score``` for the NMI and ```sklearn.cluster.KMeans``` for kmeans. In both cases, you can use your own implementation from Handin 1 or the exercises, but it will not give you extra points.  \n",
    "\n",
    "<span style='color: green'>**\\[Describe\\]**</span> which of the method performs the best and whether the results show similarities between the two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.3 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> how you would conceptionally expand the way of embedding a graph, if you had a multi-label-graph. E.g. meaning you have multiple labels and each edge needs to have exacrly one of those. So you can also have multiple edges between the same nodes, as long as they have different labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2 (8 points)\n",
    "\n",
    "**This is a hard exercise. Do it for fun or only if you are done with easier questions.**\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a new GCN that optimizes for modularity. The loss function takes in input a matrix $C \\in \\mathbb{R}^{n\\times k}$ of embeddings for each of the nodes. \n",
    "$C$ represents the community assignment matrix, i.e. each entry $C_{ij}$ contains the probability that node $i$ belong to community $j$. \n",
    "\n",
    "The loss function is the following\n",
    "$$\n",
    "loss = - Tr(C^\\top B C) + l\\|C\\|_2\n",
    "$$ \n",
    "where $B$ is the modularity matrix that you will also implement, and $l$ is a regularization factor controlling the impact of the $L_2$ regularizer. \n",
    "We will implement a two-layer GCN similar to the one implemented in the exercises, but the last layer's activation fucntion is a Softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykeen\n",
    "\n",
    "# Adjacency matrix\n",
    "G     = email.S_undir.copy()\n",
    "A     = np.array(nx.adj_matrix(G, weight=None).todense())\n",
    "I     = np.eye(A.shape[0])\n",
    "A     = A + I # Add self loop\n",
    "\n",
    "# Degree matrix\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Normalized Laplacian\n",
    "    \n",
    "# Create input vectors\n",
    "\n",
    "### TODO your code here\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float, requires_grad=True) # Indicate to pytorch that we need gradients for this variable\n",
    "As = torch.tensor(A, dtype=torch.float)\n",
    "L = torch.tensor(L, dtype=torch.float)  # We don't need to learn this so no grad required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GCN\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, L, input_features, output_features, activation=F.relu):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                L:               The \"Laplacian\" of the graph, as defined above\n",
    "                input_features:  The size of the input embedding\n",
    "                output_features: The size of the output embedding \n",
    "                activation:      Activation function sigma\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ### TODO Your code here\n",
    "\n",
    "        \n",
    "        ### TODO Your code here\n",
    "     \n",
    "    def forward(self, X):\n",
    "        ### TODO Your code here\n",
    "\n",
    "        \n",
    "        ### TODO Your code here\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the modularity matrix and the modularity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_matrix(A):\n",
    "    B = None\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return torch.tensor(B, dtype=torch.float)\n",
    "\n",
    "def modularity_loss(C, B, l = 0.01): \n",
    "    ''' Return the modularity loss\n",
    "\n",
    "        Args:\n",
    "            C: the node-community affinity matrix\n",
    "            B: the modularity matrix\n",
    "            l: the regularization factor\n",
    "            \n",
    "        :return the modularity loss as described at the beginning of the exercise\n",
    "    '''\n",
    "    loss = 0\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute labels from communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute labels from communities\n",
    "labels = None\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### Encode the labels with one-hot encoding\n",
    "def to_categorical(y):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    num_classes = np.unique(y).size\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "\n",
    "y, classes = encode_label(labels)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# Define convolutional network\n",
    "in_features, out_features = X.shape[1], classes.size # output features as many as the number of classes\n",
    "hidden_dim = 16\n",
    "\n",
    "# Stack two GCN layers as our model\n",
    "# nn.Sequential is an implicit nn.Module, which uses the layers in given order as the forward pass\n",
    "gcn = nn.Sequential(\n",
    "    GCNLayer(L, in_features, hidden_dim),\n",
    "    GCNLayer(L, hidden_dim, out_features, None),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the unsupervised model once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 100\n",
    "epochs = 2000\n",
    "\n",
    "def train_model(model, optimizer, X, B, epochs=100, print_every=10, batch_size = 2):\n",
    "    for epoch in range(epochs+1):\n",
    "        y_pred = model(X)\n",
    "        loss = modularity_loss(y_pred, B, l=l)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch:2d}, loss={loss.item():.5f}')\n",
    "\n",
    "B = modularity_matrix(A)\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
    "train_model(gcn, optimizer, X, B, epochs=epochs, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model using NMI. Since the initialization is random train the model 10 times and take the average NMI. Assign each node to the community with the highest probability. \n",
    "You should obtain an Average $\\text{NMI}\\approx0.5$.\n",
    "\n",
    "Plot the last graph with the nodes colored by commnitieis communities using ```plot_graph``` below. \n",
    "\n",
    "**Note**: You have to create the model 5 times otherwise you are keeping training the same model's parameters! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "def plot_graph(G, y_pred): \n",
    "    plt.figure(1,figsize=(15,5))\n",
    "    pos = nx.spring_layout(G)\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color=y_pred, node_size=100, cmap=plt.cm.jet)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "### YOUR CODE STARTS HERE \n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
